\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{changepage}

\begin{document}

\title{Optimizing Television Content for Video on Demand, Algorithmically Detecting Opening/Closing Credits, Recaps and Bumpers in TV Shows\'{} Video Files}
\author{Master Thesis\\ \\ Niels ten Boom  \\ s4767314}

\date{\vspace{-3ex}}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}
Viewing rates for TV are dropping gradually while the user counts of streaming services such as Netflix and Videoland are growing year over year. This shift from TV to Video on Demand introduces new problems for broadcasters (in this case RTL), as they now have to support a hybrid form of traditional broadcasting and Video On Demand (VOD). RTL does this by putting their content on Videoland after it has aired on TV. 

Video content is optimized for just one form, and currently that is TV. This means that the video content contains recaps, opening credits, bumpers (to ease a viewer into commercials) and closing credits. A viewer watching this content back-to-back may find it preferable to be able to skip these recurring segments to improve their viewing experience. Providing this functionality to users, requires metadata on where the skippable segments occur in the videos. For a large percentage of their content, such metadata has not been retained. Videoland currently hosts over 1000 different titles consisting of multiple seasons and episodes and this selection is prone to change. An automated solution that can detect these segments in videos would be very useful to solve that problem and transform the video content into a format suited for VOD. In this thesis we will explore the best methods for such an automated solution. 
% Definities van series?

There is little margin for error with this solution. The accuracy of the end result should be sufficiently high enough for it to be trusted to label all of the content automatically. If that is not the case then it needs double checking by a human. Because if a part of the video gets mislabeled, a user may skip over actual content. It is critical to RTL that this does not happen.

This research presents my findings on detecting recaps, opening credits, bumpers and closing credits given the video files from a TV-show. Because this being looked at from a business perspective, the solution should also be cost effective, meaning it should be efficient. The research question then reads:
\newline
\begin{adjustwidth}{0.3in}{0.3in}
\textit{\textbf{To what extent is it possible to accurately detect recaps, opening credits, bumpers and closing credits in video files and what are the most efficient methods? \newline}}
\end{adjustwidth}
This thesis aims to answer aforementioned research question. The rest of this section will be used to define the different type of video classes which this research is focused on. Section \ref{relatedwork} explains all of the related work. In section \ref{data} the data is presented. Section \ref{methodology} expands on the methodology and then the produced results are presented in section \ref{results}. Lastly, the results are discussed in section \ref{discussion}.

\subsection{Segment classes} \label{section:segmentclasses}
In this section we will give a background on the different types of segments to be detected. What all the segment classes have in common is that they reappear (partially) in previous or future episodes. We will use this attribute for detection. 

\subsubsection{Closing Credits}
The closing credits at the end of a video contain a lot of scrolling text most of the time and there is little variation between the frames. In these texts everybody related to the production of the video is mentioned. In general the frames all have a black background with white text. But backgrounds can also vary as can be seen in figure \ref{closingcredits}, therefore a solution that simply detects black and white would not suffice.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/screencreditslarge.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/screencreditslarge2.png}
  \end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
	\includegraphics[width=\textwidth]{images/diffcredits.png}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
	\includegraphics[width=\textwidth]{images/diffcredits2.png}
\end{subfigure}
  \caption{Examples of varying types of closing credits}
  \label{closingcredits}
\end{figure}

\subsubsection{Opening credits}
The opening credits of a TV-show are generally the same for all of the episodes in the same season. It opens the show with a theme song, presenting the most important actors at the start. If the sequence were known then it would be a rather simple problem to solve, by matching this sequence with all of the videos to locate the opening credits. The difficult part here is that they start somewhere at the start of a video, never right at the start. There is also no prior knowledge on how the opening credits of a show look like and they often change every season.

\subsubsection{Recaps}
A television show may contain recaps before the opening credits. In the recaps content of previous episodes is repeated to refresh the viewer's memory. This is useful for linear TV when an episode is aired every week. A binge watcher ideally wants to skip the recaps together with the opening credits because they have seen the content recently. Not all material preceding the opening credits is a recap though, sometimes it is original content. Classifying whether the video part before the opening credits consists of recaps is important for a fully automated solution.

\subsubsection{Bumpers}
The bumper is a part of the video that eases a viewer into the upcoming commercial (see figure \ref{examplebumper}). Most of the times it has a voice over and text on screen saying: 'Next' (or the dutch translation of 'next') and some footage of what is to be expected after the commercial break is shown.

\begin{figure}[H]
    \includegraphics[width=7.5cm]{images/straks.png}
    \centering
    \caption{Example 'Next' bumper of the dutch television show Expeditie Robinson.}
    \label{examplebumper}
\end{figure}

\subsubsection{Preview}
A preview is a segment at the end of an episode where an advance showing of fragments of the next episode(s) are played. Previews can typically be found in content that was created specially for broadcast TV. It gives the viewer a taste of what to expect in the next episode that will air a week later. However, this part is not of interest to a binge watcher.

\subsection{Definitions and Techniques}
This section will be used to elaborate on some topics mentioned in this thesis.

We define a season with multiple episodes of a TV-show as T.
\[T = \{E_1, E_2, \dots, E_x\}\]
An episode within a season is defined as a set of frames.
\[E = \{f_1, f_2, \dots, f_x\}\]


\subsubsection{Image Retrieval}
Image retrieval is a subset of the research field Information Retrieval (IR). Very generally speaking it investigates the problem where one has a query \textbf{q} expressing an information need and wants to find the best possible match for this \textbf{q} in a set of documents \textbf{D}.

In the case of image retrieval the query consists of an image for which the best matching image should be found in a document set of images. There are two approaches for doing this.

\begin{figure}[H]
	\includegraphics[width=8cm]{images/imageretrieval.png}
	\centering
	\caption{Visual example of image retrieval.}
	\label{fig:imageretrieval}
\end{figure}

\textbf{Text-based image retrieval} refers to retrieval of images based on textual metadata associated with these images. It matches images based on for instance their titles, keywords and more. The downside of this method is that if there is no metadata available then it needs to be added manually. With the growing sizes of image databases this can become quite inefficient, hence the more focus on content-based image retrieval in past years \cite{rajam2013survey}.

\textbf{Content-based image retrieval} is retrieval based on the content of the image rather than the metadata as is the case with concept-based image retrieval. Computer vision is used to evaluate the image similarity, this can be done by looking at colors, shapes, textures and more. The advantage of content based image retrieval is that it does not rely on the metadata of images.

\subsubsection{Color Histograms}
A color histogram is a representation of the distribution of colors in an image. Color histograms are a flexible and low dimensional way of representing images. A color histogram can be computed by counting the number of pixels in a certain color range, the size of this range is variable, called the bin size. The higher the bin size, the lower the dimensions of the resulting histogram. See figure \ref{fig:colorhistogram} for an example color histogram where the bin size is 1, so each color is counted. 

\begin{figure}[H]
	\includegraphics[width=8cm]{images/colorhistogram.png}
	\centering
	\caption{Color histograms with binsize n=1.}
	\label{fig:colorhistogram}
\end{figure}

Color histograms are going to be used to compute image similarity to match similar frames later in this thesis.

\subsubsection{Shot Change Detection}

Shot detection is a technique that can be used on videos to determine shot boundaries, see figure \ref{shotchange} for an example of such a shot change. 

\begin{figure}[H]
	\includegraphics[width=12cm]{images/shotchange.jpg}
	\centering
	\caption{Example of a shot change.}
	\label{shotchange}
\end{figure}

Shot detection is going to be used and thus will we expand on it. A lot of research has been done related to shot detection \cite{lienhart1998comparison} and many techniques have been proposed. 

This thesis used a method that looks at the shift in mean color in HSV space \cite{shao2015shot}, see figure \ref{hsvspace} for a visual depiction of HSV color space. HSV (Hue, Saturation, Value) color space is a more intuitive color mixing model compared to RGB (Red, Green, Blue) color space. Because one can change a value in either of the three values in HSV and expect what the new color is going to look like. This is almost impossible for RGB because for this same color change to happen, you need to change all three red, green and blue values to result in the same color space.

\begin{figure}[H]
	\includegraphics[width=5cm]{images/hsv.png}
	\centering
	\caption{The HSV color space.}
	\label{hsvspace}
\end{figure}

The shot boundaries are classified by calculating the difference in hue, saturation and value for each frame and then from this the mean is calculated. If the mean is higher than a threshold T, then a shot boundary is marked.

\section{Related Work} \label{relatedwork}

No prior work exists that explores a solution for the problem previously described (automatic labeling of segments).  A lot of related work exists that focuses on commercial or repeated sequence detection in large broadcast streams. These detection methods can be divided into three groups: fingerprint, feature-based and unsupervised methods. 

%fingerprint
With fingerprint methods the work sets up a database of fingerprints of known commercials or repeated sequences to detect these in a broadcast stream. Lienhart et al.\ \cite{lienhart1997detection} propose a method based on features to roughly localize advertisements in a stream and then construct a fingerprint database based on color coherence vectors. Gauch et al.\ \cite{gauch2006finding} propose a method based on the color moments in a stream. Covell et al.\ \cite{covell2006advertisement} implement a fingerprinting method based on audio with visual verification after a proposed match. The disadvantage of these fingerprinting methods is the setting up and maintenance of such a fingerprint database. 

%fingerprint methodes geven vaak ook methode voor repeated sequence detection maar met onvoldoende precisie

%feature based
The feature-based methods detect commercials based on extracted video features.  by looking at the changing color schemes to locate advertisements in a stream and then . Wang et al.\ \cite{wang2008multimodal} fuse the results of audio scene changes and textual content similarity between shots to segment programs including commercials.

%clustering
With the unsupervised methods the authors typically do a dimensionality reduction operation and then try to find repeated sequences or commercials with clustering methods. Herley et al.\ \cite{herley2006argos} convert the stream with a Discrete Cosine Transform (DCT) for dimensionality reduction and then propose an extensive probability framework to detect repeated sequences. Benezeth et al.\ \cite{benezeth2010unsupervised} use the Electronic Programme Guide (EPG) in addition to the dimensionality reduction to detect program boundaries.

%vergelijkbare dingen
Abduraman et al.\ \cite{abduraman2011unsupervised} propose a system to detect repeated sequences in streams by performing a DCT operation on all of the frames in the stream and then use a micro clustering technique to detect repeated sequences. They were also able to link the trailers to their respective programs that occur at a later point in the stream.

All of the previously mentioned works in this section never cover the specific topic of segmenting the classes mentioned in section \ref{section:segmentclasses}. The methods yielded high accuracies in the range of 90\% - 95\% precision. This work aims for higher accuracies and thus will not be replicating most of the previously mentioned methodologies, however from the aforementioned papers we conclude that a large dimensionality reduction step will be necessary to efficiently process a large dataset of videos.

Dimensionality reduction and efficient matching of large amounts of visual data is typically used within content based image retrieval, Zheng et al. give a very detailed summary of all the significant contributions from past years \cite{zheng2018sift}. Their summary covers three periods in image retrieval: Early methods, SIFT-based methods and CNN-based methods. Smeulders et al. present all the contributions of the early methods \cite{smeulders2000content}, these methods focused on looking at the color, texture and local geometry of images for retrieval. Not much later the Bag-of-words (BoW) model was proposed as a new method for image retrieval \cite{sivic2003video}. The advantage of this is that inverted indexes can be used for immediate retrieval of similar images. The BoW model paired with SIFT-descriptors \cite{lowe2004distinctive} as the feature vectors was used in image retrieval research for years \cite{nister2006scalable}\cite{philbin2007object}\cite{jegou2008hamming}\cite{jegou2010aggregating}\cite{jegou2012aggregating}. Since 2012 when the convolutional neural network was introduced \cite{krizhevsky2012imagenet} research switched to deep learning based methods for image retrieval \cite{babenko2014neural}]\cite{yue2015exploiting}\cite{tolias2015particular}.


\iffalse
%oude paper OCR op video
\cite{li2000automatic} %TODO

%fingerprints
\cite{lienhart1997detection} %commercial detection met fingerprints
\cite{covell2006advertisement} %commercial detection fingerprints

%features
\cite{gauch2006finding} %commercial detection features
\cite{wang2008multimodal} %doen programma segmentatie

%clustering
\cite{herley2006argos} %argos paper gebruikt audio en heel veel probability
\cite{berrani2008non} %micro clustering repeated sequence detection

\cite{benezeth2010unsupervised} %enige paper die iets vergelijkbaars doet, maar op basis van stream en programmagids, veel nuttige references
\cite{ibrahim2011tv} %andere vergelijkbare paper, op advertenties en met grote stream
\cite{abduraman2011unsupervised} %Ook hele vergelijkbare paper, werken ook weer met DCT en KVD maar recall en precision round 0.95
\fi




\section{Data} \label{data}

The dataset contains 81 video files originating from 18 differing seasons of tv-shows. See the appendix for a full breakdown of the data. Originally these files were in .mxf full broadcast format, this meant each file being 25GB on average. All the files were converted to 1920p .mp4 files, resulting in each file being 1 GB on average.

For each file the start and end timestamps for the recaps, opening credits and closing credits were annotated in the HH:MM:SS format in a CSV file, so that it could be loaded effectively.

These files were then manually viewed and all the relevant segments were annotated. 

\section{Methodology} \label{methodology}

\section{Results} \label{results}

\section{Discussion} \label{discussion}

\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
