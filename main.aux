\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}}
\newlabel{introduction}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Recurring Content Classes}{5}}
\newlabel{section:segmentclasses}{{1.1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Frame grabs from each recurrent content class.}\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:introductionexamples}{{1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Recaps}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Opening credits}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Bumpers}{5}}
\citation{lienhart1998comparison}
\citation{shao2015shot}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Previews}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.5}Closing Credits}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Definitions and Techniques}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Shot Change Detection}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Example of a shot change within 4 frames of a video.}\relax }}{6}}
\newlabel{shotchange}{{2}{6}}
\citation{lienhart1997detection}
\citation{gauch2006finding}
\citation{covell2006advertisement}
\citation{wang2008multimodal}
\citation{herley2006argos}
\citation{benezeth2010unsupervised}
\citation{abduraman2011unsupervised}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces \textbf  {Shot boundary detection}\relax }}{7}}
\newlabel{algorithm:shotboundary}{{1}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{7}}
\newlabel{relatedwork}{{2}{7}}
\citation{zheng2018sift}
\citation{smeulders2000content}
\citation{yu2002color}
\citation{manjunath1996texture}
\citation{sivic2003video}
\citation{lowe2004distinctive}
\citation{nister2006scalable}
\citation{philbin2007object}
\citation{jegou2008hamming}
\citation{jegou2010aggregating}
\citation{jegou2012aggregating}
\citation{krizhevsky2012imagenet}
\citation{babenko2014neural}
\citation{yue2015exploiting}
\citation{tolias2015particular}
\@writefile{toc}{\contentsline {section}{\numberline {3}Image Retrieval}{8}}
\newlabel{section:imageretrieval}{{3}{8}}
\citation{rajam2013survey}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Visual example of image retrieval.}\relax }}{9}}
\newlabel{fig:imageretrieval}{{3}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Feature vectors}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Three images of 10x10 pixels, consisting of only black and white pixels. The resulting feature vectors are $[64,36]^T$, $[0, 100]^T$ and $[25,75]^T$ respectively.\relax }}{9}}
\newlabel{featurevectorimages}{{4}{9}}
\citation{zheng2018sift}
\citation{yu2002colortexturemoments}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Example of distance in 2D between feature vectors that represent the example images. According to the chosen features, image 2 and 3 are the most alike.\relax }}{10}}
\newlabel{fig:distanceexample}{{5}{10}}
\citation{yu2002colortexturemoments}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Color Histograms}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Color histogram with a large binsize (binsize=128) for illustration.}\relax }}{11}}
\newlabel{fig:colorhistogram}{{6}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Color Texture Moments}{11}}
\citation{zheng2018sift}
\citation{tolias2015particular}
\citation{tolias2015particular}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Filters used for computing the color texture channels.}\relax }}{12}}
\newlabel{fig:filters}{{7}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}CNN Features}{12}}
\citation{beyer1999nearest}
\citation{ffmpeg}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{13}}
\newlabel{methodology}{{4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Details of the data that was used to perform the experiments with.\relax }}{13}}
\newlabel{table:datastatistics}{{1}{13}}
\citation{rmac-github}
\citation{faiss}
\citation{faiss-github}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feature Vector Construction}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Color Histograms}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Color Texture Moments}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}CNN Features}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Matching and Detection}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Diagram of the described methodology. The recurrent content detection process for episode 2 is portrayed.}\relax }}{15}}
\newlabel{fig:diagram}{{8}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Recurring Content}{15}}
\citation{lowe2004distinctive}
\citation{lowe2004distinctive}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Bumpers}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{16}}
\newlabel{results}{{5}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Recurring Content}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Visual example of the described evaluation methodology.\relax }}{17}}
\newlabel{fig:evaluation-example}{{9}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results of the recurrent content detection at varying values for the lowest percentiles of distances.\relax }}{17}}
\newlabel{table:resultsrecurring}{{2}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Bumpers}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Precision-Recall curves for each recurrent content class for the percentile values in [2.5, 20]. Data points that are in the upper right indicate good performance.\relax }}{18}}
\newlabel{fig:precision-recall}{{10}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Visual example of the described evaluation methodology for bumper detection.\relax }}{19}}
\newlabel{fig:evaluation-example-bumpers}{{11}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results of the experiments on the bumper detection for varying values of the ratio test.\relax }}{19}}
\newlabel{table:resultsbumpers}{{3}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{19}}
\newlabel{discussion}{{6}{19}}
\bibstyle{ieeetr}
\bibdata{references}
\bibcite{lienhart1998comparison}{1}
\bibcite{shao2015shot}{2}
\bibcite{lienhart1997detection}{3}
\bibcite{gauch2006finding}{4}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {8}References}{20}}
\bibcite{covell2006advertisement}{5}
\bibcite{wang2008multimodal}{6}
\bibcite{herley2006argos}{7}
\bibcite{benezeth2010unsupervised}{8}
\bibcite{abduraman2011unsupervised}{9}
\bibcite{zheng2018sift}{10}
\bibcite{smeulders2000content}{11}
\bibcite{yu2002color}{12}
\bibcite{manjunath1996texture}{13}
\bibcite{sivic2003video}{14}
\bibcite{lowe2004distinctive}{15}
\bibcite{nister2006scalable}{16}
\bibcite{philbin2007object}{17}
\bibcite{jegou2008hamming}{18}
\bibcite{jegou2010aggregating}{19}
\bibcite{jegou2012aggregating}{20}
\bibcite{krizhevsky2012imagenet}{21}
\bibcite{babenko2014neural}{22}
\bibcite{yue2015exploiting}{23}
\bibcite{tolias2015particular}{24}
\bibcite{rajam2013survey}{25}
\bibcite{yu2002colortexturemoments}{26}
\bibcite{beyer1999nearest}{27}
\bibcite{ffmpeg}{28}
\bibcite{rmac-github}{29}
\bibcite{faiss}{30}
\bibcite{faiss-github}{31}
