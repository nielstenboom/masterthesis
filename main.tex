\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{changepage}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{url}

\usepackage{geometry}
\geometry{margin=1.7in}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage[export]{adjustbox}[2011/08/13]

\usepackage[labelfont=bf]{caption}

\begin{document}

\title{Optimizing TV Content for Video on Demand, Leveraging Image Retrieval for Unsupervised Detection of Recurring Content in TV Shows\'{} Video Files}
\author{Master Thesis\\ \\ Niels ten Boom  \\ s4767314}

\date{\vspace{-3ex}}

\maketitle
\newpage

\begin{abstract}
This thesis presents research on image retrieval-based approaches for recurrent content detection in VOD TV-shows. The methods utilize feature vectors
constructed from video frames to compare similarities across TV-show episodes. It is found that such methods has potential but there remains room for improvement for true unsupervised detection.
\end{abstract}
\newpage

\tableofcontents
\newpage

\section{Introduction} \label{introduction}
Viewing rates for TV are dropping gradually while the number of users of streaming services such as Netflix and Videoland are growing rapidly year over year. This shift from TV to Video on Demand introduces new problems for broadcasters, as they now have to support a hybrid form of traditional broadcasting and Video On Demand (VOD). In this thesis we consider a method to help improve the user experience for VOD.

Video content is optimized for just one form, and currently that is TV. This means that the video content may contain recurring content consisting of: recaps, opening credits, bumpers (to ease a viewer into commercials), closing credits and previews. A viewer watching this content back-to-back may find it preferable to be able to skip this recurring content to improve their viewing experience. Providing this functionality to users, requires metadata on where the skippable content occurs in the videos. For a large percentage of content, such metadata has not been retained. Videoland currently hosts over 1000 different titles consisting of multiple seasons and episodes and this selection is always changing. An automated solution that can detect this content in videos would be very useful to transform the video content into a format suited for VOD. In this thesis we will explore methods for such an automated solution.
% Definities van series?

There is little margin for error with this solution. The accuracy of the end result should be sufficiently high enough for it to be trusted to label all of the content automatically. If that is not the case then it needs double checking by a human. Because if a part of the video gets mislabeled, a user may skip over actual content. It is critical to RTL that this does not happen.

This thesis presents research on unsupervised detection of recaps, opening credits, bumpers, closing credits and previews given the video files from a TV-show. We are going to attempt to tackle this problem by using image retrieval techniques. The motivation behind this is that the overlapping characteristic of the segments is that they reoccur. To be able to detect recurring content we are going to compare similarities of frames across content, this should be done efficiently and accurately, both attributes are important in image retrieval research. Our main research question then is:
\newline
\begin{adjustwidth}{0.3in}{0.3in}
\textit{\textbf{How well do image retrieval methods perform in accurately detecting recurring content across a TV-show?\newline}}
\end{adjustwidth}
With the subquestions: \\
\begin{adjustwidth}{0.3in}{0.3in}
	\textit{\textbf{How can image retrieval be used to detect recurring content?\newline}}\\
	\textit{\textbf{How accurate is each image retrieval approach?\newline}}\\
	\textit{\textbf{How can the best method be utilized in practice?\newline}}
\end{adjustwidth}
This thesis aims to answer aforementioned research questions. Our hypothesis is that we can get near perfect retrieval results with such methods. The rest of this section will be used to expand on the different type of video classes which this research is focused on. Section \ref{relatedwork} explains all of the related work. In section \ref{section:imageretrieval} image retrieval and the used methods are expanded upon. Section \ref{methodology} describes the data and methodology and then the produced results are presented in section \ref{results}. Lastly, the results are discussed and a conclusion is drawn in section \ref{discussion}.

\subsection{Recurring Content Classes} \label{section:segmentclasses}
In this section we will give a background on the different types of content to be detected. What all the content classes have in common is that they (partially) reappear in previous or future episodes. We will use this attribute for the detection. 

\subsubsection{Closing Credits}
The closing credits at the end of a video contain a lot of scrolling text most of the time and there is little variation between the frames. In these texts everybody related to the production of the video is mentioned. In general the frames all have a black background with white text. But backgrounds can also vary as can be seen in figure \ref{closingcredits}, therefore a solution that simply detects black and white would not suffice.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/screencreditslarge.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/screencreditslarge2.png}
  \end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
	\includegraphics[width=\textwidth]{images/diffcredits.png}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
	\includegraphics[width=\textwidth]{images/diffcredits2.png}
\end{subfigure}
  \caption{\textbf{Examples of varying types of closing credits}}
  \label{closingcredits}
\end{figure}

\subsubsection{Opening credits}
The opening credits of a TV-show are generally the same for all of the episodes in the same season. It typically opens the show with a theme song, presenting the most important actors at the start. If the sequence is known beforehand then it would be a rather simple problem to solve, by matching this sequence with all of the videos to locate the opening credits. The difficult part here is that they start somewhere at the start of a video, never right at the start. There is also no prior knowledge on how the opening credits of a show look like and they often change every season.

\subsubsection{Recaps}
A television show typically has recaps before the opening credits. In the recaps content of previous episodes is repeated to refresh the viewer's memory. This is useful for linear TV when an episode is aired every week. Someone watching episodes back-to-back ideally wants to skip the recaps together with the opening credits because they have seen the content recently. Not all material preceding the opening credits is a recap though, sometimes it is original content.

\subsubsection{Bumpers}
The bumper is a part of the video that eases a viewer into the upcoming commercial (see figure \ref{examplebumper}). Most of the times it has a voice over and text on screen saying: 'Next' (or the dutch translation of 'next') and some footage of what is to be expected after the commercial break is shown.

\begin{figure}[H]
    \includegraphics[width=7.5cm]{images/straks.png}
    \centering
    \caption{\textbf{Example 'Next' bumper of the dutch television show Expeditie Robinson.}}
    \label{examplebumper}
\end{figure}

\subsubsection{Previews}
A preview is a segment at the end of an episode where an advance showing of fragments of the next episode(s) are played. Previews can typically be found in content that was created specially for broadcast TV. It gives the viewer a taste of what to expect in the next episode that will air a week later. However, this part is not of interest to someone watching episodes back-to-back.

\subsection{Definitions and Techniques}
This section will be used to elaborate on some topics mentioned in this thesis. We define a season with multiple episodes of a TV-show as T.
\[T = \{E_1, E_2, \dots, E_x\}\]
An episode within a season is defined as a set of frames.
\[E = \{f_1, f_2, \dots, f_y\}\]
The videos contain shot boundaries, we refer the set of locations of shotboundaries as B.
\[B = \{b_1, b_2, \dots, b_z\}\]

\subsubsection{Shot Change Detection}

Shot detection is a technique that can be used on videos to determine shot boundaries, see figure \ref{shotchange} for an example of such a shot change. 

\begin{figure}[H]
	\includegraphics[width=12cm]{images/shotchange.jpg}
	\centering
	\caption{\textbf{Example of a shot change within 4 frames of a video.}}
	\label{shotchange}
\end{figure}

Shot detection is going to be used and thus will we expand on it. A lot of research has been done related to shot detection \cite{lienhart1998comparison} and many techniques have been proposed. 

We use a method that looks at the shift in mean color in HSV space \cite{shao2015shot}. HSV (Hue, Saturation, Value) color space is a more intuitive color mixing model compared to RGB (Red, Green, Blue) color space. Because one can change a value in either of the three values in HSV and expect what the new color is going to look like. This is almost impossible for RGB because for this same color change to happen, you need to change all three red, green and blue values to result in the same color space.

The shot boundaries are classified by calculating the difference in hue, saturation and value for each frame and then from this the mean is calculated. If the mean is higher than a threshold H, then a shot boundary is marked, refer to algorithm \ref{algorithm:shotboundary} for a pseudo-code representation.

\begin{algorithm}[H] 
	\SetAlgoLined
	shotBoundaries = List()\;
	previousMeanHSV = getMeanHSV(Episode[0])\;
	\For{frame in Episode}{
		meanHSV = getMeanHSV(frame)\;
		\If{abs(meanHSV - previousMeanHSV) $>$ H}{
			shotBoundaries.add(frame)\;
		}
		previousMeanHSV = meanHSV\;
	}
	\caption{\textbf{Shot boundary detection}}
	\label{algorithm:shotboundary}
\end{algorithm}

\section{Related Work} \label{relatedwork}

No prior work exists that explores a solution for the problem previously described (unsupervised detection of these specific recurring segments). Related work exists that focuses on commercial or repeated sequence detection in large broadcast streams. These detection methods can be roughly divided into three groups: fingerprint, feature-based and unsupervised methods. 

%fingerprint
Fingerprint methods set up a database of fingerprints of known commercials or repeated sequences to detect these in a broadcast stream. Lienhart et al.\ \cite{lienhart1997detection} propose a method based on features to roughly localize advertisements in a stream and then construct a fingerprint database based on color coherence vectors. Gauch et al.\ \cite{gauch2006finding} propose a method based on constructing feature vectors from color moments in a stream. Covell et al.\ \cite{covell2006advertisement} implement a fingerprinting method based on audio with visual verification after a proposed match. The disadvantage of these fingerprinting methods is that it is not unsupervised and the setting up and maintenance of such a fingerprint database requires much work.

%fingerprint methodes geven vaak ook methode voor repeated sequence detection maar met onvoldoende precisie

%feature based
The feature-based methods detect commercials based on extracted video features. Wang et al.\ \cite{wang2008multimodal} fuse the results of audio scene changes and textual content similarity between shots to segment programs including commercials.

%clustering
With the unsupervised methods the authors typically do a dimensionality reduction operation and then try to find repeated sequences or commercials with clustering methods. Herley et al.\ \cite{herley2006argos} convert the stream with a Discrete Cosine Transform (DCT) to reduce dimensionality and then propose an extensive probability framework to detect repeated sequences. Benezeth et al.\ \cite{benezeth2010unsupervised} use the Electronic Programme Guide (EPG) in addition to the dimensionality reduction to detect program boundaries.

%vergelijkbare dingen
Abduraman et al.\ \cite{abduraman2011unsupervised} propose a system to detect repeated sequences in streams by performing a DCT operation on all of the frames in the stream and then use a micro clustering technique to detect repeated sequences. They were also able to link the trailers to their respective programs that occur at a later point in the stream.

All of the previously mentioned works in this section never cover the specific topic of segmenting the classes mentioned in section \ref{section:segmentclasses} or use other properties only available to broadcast streams such as the EPG. The methods also focused  on recurrent content detection are exclusively based on detection of moderate to long sequences, it does not cover short recurrent sequences stitched together such as a recap. The methods yielded high accuracies in the range of 90\% - 95\% precision, also most research is from many years ago and work that focuses on repeated sequence detection in broadcast streams do not vary much in methodology. This work is not focused on broadcast streams and aims for higher accuracies and thus will not be replicating the previously mentioned methodologies, however from the aforementioned papers we conclude that a large dimensionality reduction step will be necessary to efficiently process a large visual data set.
\\

Dimensionality reduction and efficient matching of large amounts of visual data is important within content based image retrieval, Zheng et al. give a very detailed summary of all the significant contributions from past years \cite{zheng2018sift}. Their summary covers three periods in image retrieval: Early methods, SIFT-based methods and Convolutional Neural Network (CNN) based methods. Smeulders et al. present all the contributions of the early methods \cite{smeulders2000content}, these methods focused on looking at the color, texture and local geometry of images for retrieval \cite{yu2002color,manjunath1996texture}. Not much later the Bag-of-words (BoW) model was proposed as a new method for image retrieval \cite{sivic2003video}. The advantage of this is that inverted indexes can be used for immediate retrieval of similar images. The BoW model paired with SIFT-descriptors \cite{lowe2004distinctive} as the feature vectors was used in image retrieval research for years \cite{nister2006scalable,philbin2007object,jegou2008hamming,jegou2010aggregating,jegou2012aggregating}. Since 2012 when the convolutional neural network was introduced \cite{krizhevsky2012imagenet} research switched to CNN-based methods for image retrieval because they achieved better performance on several image retrieval tasks	 \cite{babenko2014neural,yue2015exploiting, tolias2015particular}.


\iffalse
%oude paper OCR op video
\cite{li2000automatic} %TODO

%fingerprints
\cite{lienhart1997detection} %commercial detection met fingerprints
\cite{covell2006advertisement} %commercial detection fingerprints

%features
\cite{gauch2006finding} %commercial detection features
\cite{wang2008multimodal} %doen programma segmentatie

%clustering
\cite{herley2006argos} %argos paper gebruikt audio en heel veel probability
\cite{berrani2008non} %micro clustering repeated sequence detection

\cite{benezeth2010unsupervised} %enige paper die iets vergelijkbaars doet, maar op basis van stream en programmagids, veel nuttige references
\cite{ibrahim2011tv} %andere vergelijkbare paper, op advertenties en met grote stream
\cite{abduraman2011unsupervised} %Ook hele vergelijkbare paper, werken ook weer met DCT en KVD maar recall en precision round 0.95
\fi


\section{Image Retrieval} \label{section:imageretrieval}
Image retrieval is a subset of the research field Information Retrieval (IR). Explained briefly, it investigates the problem where one has a query \textbf{q} expressing an information need and wants to find the best possible match for this \textbf{q} in a set of documents \textbf{D}.

In the case of image retrieval the query consists of an image for which the best matching image should be found in a document set of images. There are two approaches for doing this.

\begin{figure}[H]
	\includegraphics[width=8cm]{images/imageretrieval.png}
	\centering
	\caption{\textbf{Visual example of image retrieval.}}
	\label{fig:imageretrieval}
\end{figure}

\textbf{Text-based image retrieval} refers to retrieval of images based on textual metadata associated with these images. It matches images based on for instance their titles, keywords and more. The downside of this method is that if there is no metadata available then it needs to be added manually. With the growing sizes of image databases this can become quite inefficient, hence the more focus on content-based image retrieval in past years \cite{rajam2013survey}.

\textbf{Content-based image retrieval} is retrieval based on the content of the image rather than the metadata as is the case with concept-based image retrieval. Computer vision is used to evaluate the image similarity, this can be done by looking at colors, shapes, textures and more. The advantage of content based image retrieval is that it does not rely on the metadata of images and thus does not require manual labeling.

\subsection{Feature vectors}
%TODO: veranderen in kleurencounts wit en zwart
A feature vector is a vector containing a numeric representation of multiple characteristics of an object. In this case the objects will be images, the frames of a video. To illustrate feature vectors for images, a small example will be given. Consider the following three images of 10 by 10 pixels consisting only of black or white pixels in figure \ref{featurevectorimages}: 

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.2\textwidth}
		\includegraphics[width=\textwidth]{images/mostlywhite.png}
	\end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
		\includegraphics[width=\textwidth]{images/allblack.png}
	\end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
		\includegraphics[width=\textwidth]{images/mostlyblack.png}
	\end{subfigure}
	\caption{\textbf{Three images of 10x10 pixels, consisting of only black and white pixels. The resulting feature vectors are $[64,36]^T$, $[0, 100]^T$ and $[25,75]^T$ respectively.}}
	\label{featurevectorimages}
\end{figure}

If we would like to measure the image similarity between these pictures then we could do this on a pixel-per-pixel basis, this could work for low dimensional images. But comparing HD images consisting of 1920 pixels by 1080 pixels would require massive computation and will require extensive logic to handle small deviations between images. This can be resolved by creating feature vectors to represent the images. A representation for these example images could be the number of black and white pixels in the image. This results in the feature vector $\textbf{x} = [\# white pixels, \# black pixels]^T$. This vector can now be represented in 2D-space, and compared to other vectors in 2D space by calculating the distance between other points in that space. The distance is computed by calculating the euclidean distance between each vector.

\[d(\textbf{p,q}) = \sqrt{(p_1 - q_1)^2 + \dots + (p_n - q_n)^2} \]

In figure \ref{fig:distanceexample} the plotted feature vectors and the distances between them can be viewed. Example images 2 and 3 are the most alike because the distance between them is the smallest. 

\begin{figure}[H]
	\includegraphics[width=8cm]{images/distanceexample.png}
	\centering
	\caption{\textbf{Example of distance in 2D between feature vectors that represent the example images. According to the chosen features, image 2 and 3 are the most alike.}}
	\label{fig:distanceexample}
\end{figure}

The similarity of images can be retrieved by computing the distance between their feature vectors. A low distance indicates a higher similarity. Counting the number of black and white pixels for the construction of feature vectors is not a very robust method. Many research in image retrieval has been done on the construction of these feature vectors for achieving the best retrieval results. Part of this research is determining the best method for the construction of the image feature vectors to apply for our problem. We will expand on the methods that are going to be explored. 

%Misschien uitwijden over local en global descriptors
As mentioned in section \ref{relatedwork}, Zheng et al. \cite{zheng2018sift} outline three significant periods in image retrieval, the early methods focused on global descriptors based on color and texture. These global descriptors were not good at handling image changes in illumination, translation, occlusion and truncation. This gave rise to local descriptors based SIFT methods until recently CNN-descriptors became the most popular. 

Initially the plan was to experiment with a method out of each of these three periods. However, early experimentation indicated that SIFT methods would be too inefficient for our application. That is why we settled on trying out color histograms as a global descriptor, local color texture moments \cite{yu2002colortexturemoments} as a method that tries to describe the image on a less global level by taking texture of the image into account. Finally CNN-descriptors as local descriptors.

This is not an exhaustive list of methods, but because computing feature vectors for a large amount of frames is resource intensive, we limited ourselves to these three. If one of the methods achieves promising results then future research can expand on variations of the method or if none of the methods achieve acceptable results, then future research could try other distinct methods.

\subsection{Color Histograms}
A color histogram is a representation of the distribution of colors in an image. Color histograms are a flexible and low dimensional way of representing images. A color histogram can be computed by counting the number of pixels in a certain color range, the size of this range is variable, called the bin size. A larger bin size results in lower dimensions of the resulting histogram. For example, if one chooses a bin size that is half of the intensity range. The resulting vector would have 6 dimensions, two bins for each color channel. See figure \ref{fig:colorhistogram} for an example with such a bin size.
\begin{figure}[H]
	\includegraphics[width=8cm]{images/colorhistogram.png}
	\centering
	\caption{\textbf{Color histogram with a large binsize (binsize=128) for illustration.}}
	\label{fig:colorhistogram}
\end{figure}
Color histograms are global descriptors because they describe the image on a global level.

\subsection{Color Texture Moments}
Color texture moments as proposed by Yu et al. \cite{yu2002colortexturemoments} are a low-level feature descriptor that integrates both color and texture characteristics of an image. An image is converted from RGB to HSV color space, and on each color map a 2d convolution is applied for each 3x3 filter shown in figure \ref{fig:filters}.

\begin{figure}[H]
	\includegraphics[width=5cm]{images/filters.png}
	\centering
	\caption{\textbf{Filters used for computing the color texture channels.}}
	\label{fig:filters}
\end{figure}

Which results in 8 channels for each color map, 24 channels in total. For each of the resulting channels the first and second color moments, which are the mean and standard deviation, are taken. This results in a 48-dimensional feature vector.

\subsection{CNN Features} 
Convolutional neural networks are very good at image classification and segmentation tasks. However it was found that using the resulting channels after the convolution layers as feature vectors also perform well at image retrieval tasks. There are many ways to construct CNN feature vectors that perform well as described in \cite{zheng2018sift}.

We will use the Regional Maximum Activation of Convolutions (R-MAC) method introduced by Tolias et al. \cite{tolias2015particular} as our implementation for the feature vectors. The frames are fed through a pre-trained VGG16 neural network. The last fully connected layers of the network are discarded, and the resulting channels with activations from the convolutional layers are divided up into regions. For each of these regions the Maximum Activations of Convolutions (MAC) feature vector is calculated, and all these feature vectors are then post-processed by applying L2-normalization, PCA-whitening and L2-normalization. Then all the feature vectors are summed into one resulting feature vector and L2-normalization is applied as the final step. For a more extensive description, one can refer to \cite{tolias2015particular}.

\section{Methodology} \label{methodology}
We want to explore whether we can detect recurring content unsupervised and if so, which method of feature vector construction scores the best in terms of precision and recall. Initially a non-generalized approach was tried, where a unique method for each content class was used (For example: OCR for textual closing credits, similarity matrices for opening credits and feature based classification). However, this was soon found to be ineffective because of a high variety of edge cases in creative non-structured video content. 

The characteristic of all the segment classes is that they reoccur either in previous or future episodes. To match frames from one episode with other episodes, feature vectors were constructed from the frames and the distances between these vectors were computed and saved. If a part of the video consecutively has matching feature vectors from previous or future episodes, then that part is labeled as a recurring segment. It is therefore important that the resulting feature vectors do not mismatch between actual recurring content and content that looks alike. Methods chosen for construction of the feature vectors should therefore not result in feature vectors of very high dimensionality, because this has proven to reduce the distance between the farthest and closest points \cite{beyer1999nearest}, likely diminishing the distinctive property of the feature vectors that is needed.

We will do separate experiments for the bumper detection as they require a different approach for detection because they only contain recurring content from within the episode itself.

All of the experiments were coded in Python and executed on AWS m4.2xlarge instances.

\subsection{Data}
The data set contains 80 video files originating from 16 different seasons of tv-shows, amounting to around 60 hours of content. Of most seasons only the first few episodes will be looked at to have a more varied data set. Originally these files were in .mxf full broadcast format, this meant each file being 25GB on average. All the files were converted to .mp4 files having a width of 320 pixels width and varying lengths, to maintain the aspect ratio. The resizing step was done to drastically decrease the computing time needed. The conversion was done with FFmpeg \cite{ffmpeg}.

For each file the start and end timestamps for the recaps, opening credits, closing credits and previews were annotated in the HH:MM:SS format in a CSV file, so that it could be loaded and compared effectively.

\begin{adjustbox}{center}
	\begin{tabular}{ c | c c c c c c} 
			& recaps	& openingcredits	& closingcredits	& previews	& recurring	& total \\
		\hline
		total seconds 	& 3275	& 4669	& 690	& 592	& 9423	& 188981 \\
		\% of recurring	& 34.8\%& 49.5\%& 7.3\% & 6.3\% & -		& - \\
		\% of total 	& 1.7\%	& 2.5\%	& 0.4\%	& 0.3\%	& 4.9\%	& - \\
	\end{tabular}
\end{adjustbox}


\subsection{Feature Vector Construction}
Video generally plays at 25 frames per second, frames very close to each other should only have very slight variations. That is why frames every 3 frames or just shotboundary frames will be taken into account to drastically reduce the computing complexity. We take all the video files of a season $E_x \in T$ and convert each file to a set of feature vectors $S_x = \{v_1, v_2, \dots, v_l\}$ with the function $f$.

\[f(E_x) = S_x = \{v_1, v_2, \dots, v_l\}\]

The rest of the subsections will expand on different implementations of the function $f$, how the different types of feature vectors are constructed. We use three different methods for the feature vector construction. These methods were chosen because of their efficient computation and accuracy in image retrieval tasks. The methods chosen were color histograms, local color texture moments and CNN features.

\subsubsection{Color Histograms}
For the construction of the color histograms the pixels intensities in each channel (Red, Green, Blue (RGB)) are counted and binned according to a specified bin size. All these bins will be concatenated to result in a feature vector, the size of the feature vector is a result of the chosen bin size.

We use 30 bins to represent a single color channel. These are then concatenated into a 1D vector, resulting in a feature vector with a dimensionality of 90. The values are normalized by dividing each bin with the sum of all bins.

\[H = \frac{H_x}{\sum H_i} \]

A histogram is computed for every 5 frames in the video and then stored and saved in a list that preserves the order.

\subsubsection{Color Texture Moments}
TODO

\subsubsection{CNN Features}
For the computation of the CNN feature vectors we use a Github re-implementation written in Python \cite{rmac-github}.


\subsection{Matching and Detection}

All the resulting lists of feature vectors will be added to the index of an empty Faiss instance. Faiss is a library for efficient similarity search of dense vectors \cite{faiss, faiss-github} and thus perfectly suited for our task. This library will efficiently handle building the inverted index and nearest neighbor matching. The science that is behind large scale matching of dense vectors is a research area on its own, but outside the scope of this thesis. We use Faiss because it is released as an open source library and therefore easy to use in an implementation.

\begin{figure}[H]
	\includegraphics[width=\paperwidth, center, scale=0.7]{images/thesisdiagram.png}
	\centering
	\caption{\textbf{Diagram of the described methodology. The recurrent content detection process for episode 2 is portrayed.}}
	\label{fig:diagram}
\end{figure}

All distances up until the 5th percentile will be taken into consideration. This is because on average 3.7\% of the content was skippable for the data that was used.

\subsubsection{Bumper Detection}
The methodology for detecting the bumpers within video content is different. Because a bumper only contains recurring content from within the video itself. It shows shots that will be shown after the break, these shots may not reoccur in other episodes within a season. Therefore we cannot use the same methodology as described before for detection.

For the detection of bumpers we take a sliding window of 20 seconds of video frames, this will be the query. The index will consist of the remaining frames in the video, the resulting distances of the two nearest neighbors will be saved and then this process is repeated for the next 20 seconds in the video.

After all the closest in-episode nearest neigbors are computed for each frame then from these results bumpers can be detected. We found that a global threshold for determining matches does not work well, probably because of the fact that bumpers are very short. Therefore we will use the ratio test described by Lowe \cite{lowe2004distinctive} for deciding on whether there is a match. With the ratio test the distance between the closest neighbor and second-closest neighbor is taken into account. The ratio of the distance of the closest neighbor to the distance of the second-closest neighbor determines whether there is a correct match, if the ratio is low then there is a higher chance for the match to be correct. In \cite{lowe2004distinctive} they classify a match to be correct if the ratio of the distances is equal to 0.8 or lower, we will experiment with a ratio $r \in \{0.5, 0.6, 0.7, 0.8\}$ to find the optimal ratio to apply for this problem.



\section{Results} \label{results}

The results will be evaluated as a retrieval problem. By doing so, the measures precision and recall could be used to describe several aspects of each method. Each second of recurrent content is marked as a relevant instance and if recurrent content gets detected, then those are relevant detected seconds, by doing so we can calculate precision and recall as following:

\[\mathrm{precision} = \frac{\mathrm{detected\;relevant\;seconds}}{\mathrm{detected\;seconds}}\]

\[\mathrm{recall} = \frac{\mathrm{detected\;relevant\;seconds}}{\mathrm{total\;relevant\;seconds}}\]

Because it is possible for the manual annotation to also be slightly inaccurate, we mark detections that are within a 2 second margin of the relevant section, to also be relevant.

\begin{center}
	\begin{tabular}{ c c c c c c c} 
		\hline
		\textbf{Method} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} \\
		\hline
		 & \multicolumn{2}{c}{5\% lowest} & \multicolumn{2}{c}{7.5\% lowest} & \multicolumn{2}{c}{10\% lowest} \\
		\hline
		CH180 			& \textbf{0.884} 	& 0.687		& 0.830 	& 0.812 	& 0.786	 	& 0.855 \\
		CTM 			& 0.883 	& 0.681 		& 0.841 	& 0.796 & 0.796 	& 0.853 \\
		CNN 			& 0.854		& 0.733			& 0.808		& 0.838 & 0.748		& \textbf{0.892} \\
		\hline
	\end{tabular}
\end{center}

\subsection{Bumper detection}
Because bumpers are short segments, the bumpers were evaluated in a different manner. We do not calculate precision and recall based on numbers of retrieved relevant seconds but whether a detection has overlap with a bumper. If a detection has overlap with a bumper, then this detection is marked as relevant. Precision and recall will thus be calculated according to the following formulas:

\[\mathrm{precision} = \frac{\mathrm{detections\;with\;overlap}}{\mathrm{total\;detections}}\]

\[\mathrm{recall} = \frac{\mathrm{detections\;with\;overlap}}{\mathrm{total\;bumpers\;in\;episode}}\]

\begin{center}
	\begin{tabular}{ c c c c c c c c c} 
		\hline
		\textbf{Method} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} \\
		\hline
		& \multicolumn{2}{c}{\textbf{0.5 ratio}} & \multicolumn{2}{c}{\textbf{0.6 ratio}} & \multicolumn{2}{c}{\textbf{0.7 ratio}} & \multicolumn{2}{c}{\textbf{0.8 ratio}} \\
		\hline
		CH180 			& 0.195				& 0.312 		& 0.203		& 0.562		& 0.129		& 0.646 	& 0.085 	& 0.792 \\
		CTM 			&  0.145			& 0.542 		& 0.089		& 0.667 	& 0.072		& 0.833 	& 0.072 	& 0.958 \\
		CNN 			& \textbf{0.611}	& 0.917			& 0.484		& 0.938 	& 0.288		& \textbf{0.979} 	& 0.081 	& 0.979 \\
		\hline
	\end{tabular}
\end{center}

\section{Discussion} \label{discussion}
%interpretations
The results show that for the recurrent segment detection, the results for each different method are quite close. Meaning that the way the feature vectors are constructed does not improve the results significantly. Moreover, it is noticed that the detection results are decent but not extraordinary, meaning that there is room for improvement in the methodology. 

From the results of the bumper detection we notice there is a difference in results for each method. The CNN-based method outperforms the other methods significantly. An explanation for this could be that the CNN feature vectors contain more distinctive properties then the other methods, this means that the ratio test can be used successfully. 

%implications: why do the results matter
With these results we set the first steps towards a fully automated solution for detection of recurrent content. Many research has been done on broadcast streams, this thesis presents the first results on VOD content.

%limitations: what can't the results tell us
The experiments were mostly performed on the first three episodes of a single season. However, an end-solution would take a whole season into account. Also, sometimes the recurring content spans across different seasons (recapping the previous season in the first episode for example), our experiments did also not take this attribute into account.

The bumper detection is based on how RTL inserts its bumpers into content. With our experiments, we did not test if our methodology also applies to the bumpers of other publishers.

%recommendations: future studies
Ideally, future studies could be performed on a larger dataset consisting of full seasons of several shows to further prove the validity of this method and these future studies could try variations of the methodology and different ways of constructing the feature vectors. 
\\
\\
- I did not do an extensive study on all of the TV shows, so other edge cases are possible in other TV-shows.\\
- Very similar scenes occur regardless if it is a skippable segment or not\\
- Distances can be hard to navigate\\
- Using full seasons might give different results\\
- Video content is creative, never a clear structure, can encounter weird things (Changing of opening credit scenes, dismissal of opening credit scenes, rebroadcast of same content without it being a recap)\\
- Flash forwards/backwards confuse the model\\
- Recaps consisting of shot of different camera angles not used before

Future work: \\
- Take into account previous and next seasons\\
- Different methodologies for construction of the feature vectors\\
- Same methodology on different non-RTL data\\
- Take sound into account

\section{Conclusion}
In this thesis we explored how well an image retrieval based approach could detect recurring content in a TV-show in an unsupervised way. Experiments were performed using three different kind of feature vectors. Based upon these experiments we can conclude that an image retrieval based approach definitely has potential for the unsupervised detection of recurrent content in TV-Shows. The experiments showed that the three different methods all get reasonable results. Even with the decent results, using the method for a metadata labeling task will still require manual supervision. Future research is needed to improve upon the methodology or to try other image retrieval techniques. With this research we set a baseline for the mostly unexplored research area of recurrent content detection that is not focused on TV-broadcasts but on TV-shows for VOD platforms. 

\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
